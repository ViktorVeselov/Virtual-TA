{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f02257-8958-4b02-b5e5-54d21edc2932",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### $Name:\\,\\color{blue}{Christopher\\,J.\\,Watson,\\,Joseph\\,Binny,\\,Viktor\\,Veselov}$\n",
    "##### $School:\\,\\color{blue}{Marcos\\,School\\,of\\,Engineering,\\,University\\,of\\,San\\,Diego}$\n",
    "##### $Research:\\,\\color{blue}{MSAAI\\,Machine\\,Learning\\,\\,TA}$\n",
    "##### $Date:\\,\\color{blue}{1/18/2024}$\n",
    "##### $Revision:\\,\\color{blue}{1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5618db-340c-4d72-b54a-bd72383eb14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Imports\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "from time import time\n",
    "import joblib\n",
    "\n",
    "#Transformers Library\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#GPTQ Libraries\n",
    "from optimum.gptq import GPTQQuantizer, load_quantized_model\n",
    "\n",
    "#Pipelining-Langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86eb182d-cad9-41e4-96f9-ae376055886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "# Setup CUDA\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1aa669-e0cd-4219-ad4b-491db04d0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace authentication token\n",
    "hf_auth = You-Need-Your-Own-Key(This can be kept in secrets and memory keys)\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60eea0-caf7-49f8-9cdd-874fd2ea7300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If cuda is enabled do quantizing\n",
    "if torch.cuda.is_available():\n",
    "    bnb_config = transformers.BitsAndBytesConfig(\n",
    "        load_in_4bit =True,\n",
    "        #load_in_8bit =True, # Linux Setting\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=bfloat16\n",
    "        #llm_int8_threshold = 6.0 # Linux Setting\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "# Basic Tokenizing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Setup model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=hf_auth #This line needs to correspond to your key\n",
    ")\n",
    "\n",
    "# Let's pipeline it\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4cc8b2d-0f31-4d01-a527-e0c1245c6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function the return of a basic string\n",
    "def AskMeAnything(query):\n",
    "    sequences = pipeline(\n",
    "                    query,\n",
    "                    do_sample=True,\n",
    "                    top_k=10,\n",
    "                    num_return_sequences=1,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    max_length=200,\n",
    "                    )\n",
    "\n",
    "    results = []\n",
    "    for seq in sequences:\n",
    "        results.append(seq['generated_text'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ae2b00-59e3-443c-b33c-6a9af864e17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What is data science?\\nIn today's data-driven world, data science has become an essential tool for organizations to gain insights, make informed decisions, and drive innovation. But what exactly is data science? In this article, we will explore the definition of data science, its key components, and its applications in various industries.\\n\\nDefinition of Data Science\\n\\nData science is a field that combines elements of computer science, statistics, and domain-specific knowledge to extract insights and knowledge from data. It involves using various techniques and tools to analyze, interpret, and visualize data to gain insights and make informed decisions. Data science is a multidisciplinary field that requires a combination of technical skills and domain expertise to be successful.\\n\\nKey Components of Data Science\\n\\nThere are several key components of data science that are essential to understand:\\n\\n1. Data Acquisition: This involves collecting and gathering data\"]\n"
     ]
    }
   ],
   "source": [
    "# Test output without rag for fun\n",
    "results = AskMeAnything(\"What is data science?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8d7b7e-2c93-48e5-9831-f4f77c193d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in rag data\n",
    "loader = TextLoader(\"./TA_Data/Mod1-Presentations.txt\",\n",
    "                    encoding=\"utf8\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc02c1fd-b291-4805-a358-802e55a8af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic chunking, this only works for basic text\n",
    "text_chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "chunk_stack = text_chunks.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd3e6ecd-7651-4dd9-9104-40ed54771cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Embeddings. Could be done with LLama2 embeddings but this is much more lightweight.\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": device}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a7ebf1-45a9-41d1-83cc-1381b3ad9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines the hugging face entries into langchain\n",
    "hfm = HuggingFacePipeline(pipeline=pipeline)\n",
    "\n",
    "# Spawn a RAG vector store using ChromaDB (For now on the same device in script instead of standalone instance)\n",
    "vectordb = Chroma.from_documents(documents=chunk_stack, embedding=embeddings, persist_directory=\"chroma_db\")\n",
    "# create a retriever from the database\n",
    "retriever = vectordb.as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=hfm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25424d-bd09-4c78-85ba-627e6a3c0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing RAG\n",
    "query= \"What is Diagnostic analytics?\" \n",
    "result = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "997bec43-d44d-4170-a79d-bc3e1b157bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Diagnostic analytics is a type of analytics that is used to understand why something happened in the past. It is also known as causal analytics and is used to assess posts, followers, page views, likes and reviews to identify what worked and what did not work in past campaigns. Diagnostic analytics uses statistical techniques, such as correlation, to measure the degree of relationship between linear variables.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Results\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064ae2a4-f36e-41a2-8b6f-1833288096c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Quantize the model\n",
    "#quantizer = GPTQQuantizer(bits=4, dataset=\"c4\", block_name_to_quantize = \"model.decoder.layers\", model_seqlen = 2048)\n",
    "#quantized_model = quantizer.quantize_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb62979-ca8e-4e5f-9ae8-20608b5a7b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
