{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c73f19-35d6-4ffd-a534-c7de10ed8bf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### $Name:\\,\\color{blue}{Christopher\\,J.\\,Watson,\\,Joseph\\,Binny,\\,Viktor\\,Veselov}$\n",
    "##### $School:\\,\\color{blue}{Marcos\\,School\\,of\\,Engineering,\\,University\\,of\\,San\\,Diego}$\n",
    "##### $Research:\\,\\color{blue}{MSAAI\\,Machine\\,Learning\\,\\,TA}$\n",
    "##### $Date:\\,\\color{blue}{1/18/2024}$\n",
    "##### $Revision:\\,\\color{blue}{1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f3cbc2-2c2d-458f-a486-82094e7c8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Imports\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "from time import time\n",
    "import joblib\n",
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "#Transformers Library\n",
    "import transformers\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "#Pipelining-Langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8d17c9-3d98-4300-b9a3-af6bdcf7d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49db47f4-711a-43b9-9f42-735a80ad442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New instruction dataset\n",
    "ads500B_dataset = \"current_questions.txt\"\n",
    "\n",
    "base_dir = './'\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = base_dir + 'llama-7b-ADS-500B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774b6929-3595-4746-818d-20a5886ca325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: What are the key software languages taught in ADS 500B course?\n",
      "### Assistant: The key software languages taught in ADS 500B course are Unix, SQL, R, and Python\n",
      "### Human: What is the focus of this course?\n",
      "### Assistant: This course is focused on practical aspects of exploratory data analysis, analytics, and basic machine learning techniques\n",
      "### Human: What types of questions are included in the quizzes of this course?\n",
      "### Assistant: The quizzes in this course consist of multiple choice and true and false questions\n",
      "### Human: What should students leverage to help them through the assignments and quizzes?\n",
      "### Assistant: Students should leverage the supplemental Unix material in Blackboard to help them through the assignments and quizzes\n",
      "### Human: What is recommended for students to install in preparation for Module 2?\n",
      "### Assistant: It is recommended for students to install the full graphical user interface version of Anaconda in preparation for Module 2\n"
     ]
    }
   ],
   "source": [
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding=\"cp1252\") as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def show_set(showset):\n",
    "  for s in showset:\n",
    "    print(s)\n",
    "\n",
    "dataset = read_dataset(base_dir + ads500B_dataset)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "  d = d.replace(\", ###\", \"\\n###\").strip()\n",
    "  d = d[:-1]\n",
    "  dataset[i] = d\n",
    "\n",
    "show_set(dataset[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43936423-9b62-41a0-88f1-a416046a13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit =True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b162d7-a321-4277-8530-2d96beb35763",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    new_model,\n",
    "    local_files_only=True,\n",
    "    add_special_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f983782-a205-47a6-9fff-248f4d81a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement History\n",
    "stop_list = ['\\n### Human:', '\\n```\\n', '\\n\\nQuestion:','\\n``` Human:']\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33cc686-e7be-4751-81ef-dc2c4b721d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin C:\\Users\\chris\\anaconda3\\envs\\LLMTA\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Users\\chris\\anaconda3\\envs\\LLMTA\\bin\\cudart64_12.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary C:\\Users\\chris\\anaconda3\\envs\\LLMTA\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583487f60afe4c4bbe63ec9821c8254e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    new_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f0fa4c-6ec6-4280-8c3e-5a19aae095ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the key software languages taught in ADS 500B course?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16, device_map=\"auto\", max_length=1000, stopping_criteria=stopping_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e32aa-ecc3-4502-953c-5241dddc07c7",
   "metadata": {},
   "source": [
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcbb7b8-3a11-45a8-9cf0-bca070ab8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A Function to segment responses\n",
    "inputs: \n",
    "# text - the response from the model\n",
    "'''\n",
    "def extract_first_b(text):\n",
    "    # Split the text into sections\n",
    "    sections = text.split(\"###\")\n",
    "    \n",
    "    # Loop through the sections to find the first question and answer pair\n",
    "    for section in sections:\n",
    "        if section.strip().startswith(\"Human:\"):\n",
    "            # Extract the question\n",
    "            question = section.strip().split(\"\\n\")[0][7:].strip()\n",
    "        elif section.strip().startswith(\"Assistant:\"):\n",
    "            # Extract the answer\n",
    "            answer = section.strip().split(\"\\n\")[0][10:].strip()\n",
    "            # Return the question and answer as a dictionary\n",
    "            return {'question': question, 'answer': answer}\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_first_c(text):\n",
    "    # Split the text into sections\n",
    "    text = text.replace(\"Question:\", \"\\n\\n\")\n",
    "    text = text.replace(\"\\n### Human\", \"\\n\\n\")\n",
    "    text = text.replace(\"Human:\", \"\")\n",
    "    sections = text.split(\"\\n\\n\")\n",
    "    return sections[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c2b05f-3e84-4321-8c60-a2bc82366122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 297 ms\n",
      "Wall time: 835 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": device}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5948d2de-9c2c-4a11-a5b1-ecbd7814342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"./chroma_db\"\n",
    "if not os.path.exists(store_path):\n",
    "    loader = TextLoader(\"./current_questions.txt\",\n",
    "                        encoding=\"cp1252\")\n",
    "    docs = loader.load()\n",
    "    text_chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "    chunk_stack = text_chunks.split_documents(docs)\n",
    "    vectordb = Chroma.from_documents(documents=chunk_stack, embedding=embeddings, persist_directory=\"chroma_db\")\n",
    "else:\n",
    "    vectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0abf3f3c-6312-48b1-8cf0-e7b74de7c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "hfm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=hfm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a52feda-bcbe-4ea7-8616-036fcbe47ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A Function to drive responses\n",
    "inputs: \n",
    "# system - the system prompt\n",
    "# prompt - the user question\n",
    "# pipe - the HF inference pipeline\n",
    "'''\n",
    "def prompt_driver(system, prompt, pipe):\n",
    "    load_str = f\"<s>[INST] <<SYS>> {system} <</SYS>>\\n\"\n",
    "    load_str = f\"### Human: {prompt}\"\n",
    "    #print(load_str)\n",
    "    result = pipe.invoke(load_str)\n",
    "    #result = result[0]['generated_text']\n",
    "    result = extract_first_c(result['result'])\n",
    "    return result\n",
    "\n",
    "system = \"You are a helpful AI assistant that is an assistant teacher for a class called\"\n",
    "system += \" ADS 500B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf3ab1f5-3892-428e-bb19-c30f766da843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: total: 8.81 s\n",
      "Wall time: 9.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### Assistant: Hello, I'm here to help you learn Python programming.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt =  \"Hello chatbot, I'm your beta tester\"\n",
    "result = prompt_driver(system, prompt, qa)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4d626-ba78-40d1-a379-0906f0fbe2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_interface(message, history):\n",
    "    final_string = \"I don't know\"\n",
    "    system = \"You are a helpful AI assistant that is an assistant teacher for a class called\"\n",
    "    system += \" ADS 500B\"\n",
    "\n",
    "    try:\n",
    "        result = prompt_driver(system, message, qa)\n",
    "        final_string = f\"{result}\"\n",
    "    except:\n",
    "        pass\n",
    "    return final_string\n",
    "\n",
    "# let's build the interface\n",
    "iface = gr.ChatInterface(\n",
    "    fn=chatbot_interface,\n",
    "    title=\"<div style='display: flex; align-items: center;'><img src='https://logos-download.com/wp-content/uploads/2021/01/University_of_San_Diego_Logo_full-1536x1536.png' alt='Your Image' style='margin-right: 10px; max-height: 100px'><h1 style='flex:1; margin-right: 100px; text-align: center;'>QuestionBot for ADS 500 powered by Llama 2</h1></div>\",\n",
    "    examples= [\"How is R-squared interpreted in the context of regression analysis?\", \"What are the key software languages taught in ADS 500B course?\", \"How does you save you changes and exit the file in the vi text editor?\",\n",
    "           \"How can you install `scikit-learn` if it's not already installed?,\", \"What is the focus of Module 7 in the course?\", \"Why is the R-squared value important in evaluating a linear regression model?\", \"Who made you?\"],\n",
    "    description=\"This is a demo of a chatbot that stores reference text and attempts to give an answer from there.\",\n",
    "    )\n",
    "\n",
    "iface.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
